\section{Our Approach}
\label{s:approach}

In this section, we describe and discuss our approach to dealing with dynamic data, i.e., insertions and deletions of elements to the ground set $N$ in the context of the distributed algorithm from \citet{mirzasoleiman2013distributed} described previously. Our approach is based on the simple observation that a local machine does not need to send its local solution to the central machine at every update. The goal is to minimize the communication complexity, which is the number of elements being sent between machines. As we will see, this communication complexity is minimized at the expense of the approximation ratio of the solution of the algorithm.

\subsection{Warm-up: a naive algorithm} A naive approach to insertions and deletions is to recompute a local solution $S_i$ and send this updated local solution to the central machine every time an element $e$ is inserted or deleted on the machine with part $V_i$. Of course, the issue is that insertions and deletions may happen frequently with large datasets, in which case the communication complexity of this algorithm is very poor since local solutions are constantly being sent to the central machine. Note that the approximation ratio of the non-dynamic algorithm is always maintain here since this algorithm is equivalent to rerunning the entire non-dynamic algorithm at every insertion and deletion. Can we cut down the communication complexity of this naive algorithm while maintaining a good approximation ratio? 

\subsection{A thresholding algorithm} The main idea of our algorithm is that an updated local solution $S_i$ is sent to the central machine if it has significantly changed compared to the last local solution $S_i'$ sent to the central machine from this local machine. More precisely, let $d(S,T)$ be a distance metric between sets of elements measuring how close $S$ and $T$ are. Then, when an element $e$ is inserted or deleted from $V_i$ on a local machine, a local solution $S_i$ is recomputed and if 
%
$$ d(S_i, S_i') \geq t$$
%
for some threshold $t$, then $S_i$ is sent to the central machine. 

This algorithm is general, the distance function $d(S,T)$ and the threshold $t$ can be designed as desired for different applications. Note that as the threshold $t$ increases, the communication complexity improves since local solutions are less often sent to the central machine. However, as $t$ increase, the approximation ratio worsens since the central machine has access to increasingly outdated local solutions.  The main tradeoff with this algorithm is therefore between the communication complexity and the approximation ratio. Our algorithm is designed so that this tradeoff can be tuned as desired with the threshold $t$. As we will show in Section~\ref{s:experiments}, it is possible to drastically decrease the communication complexity while maintaining a good approximation ratio for the application we consider.

\subsection{The distance function d(S,T)} As for the threshold $t$, the distance function $d(S,T)$ can be adapted for different applications and different needs. A simple and effective distance function is the hamming distance, i.e., how many elements $S$ and $T$ disagree on. More sophisticated distance functions may be dependent on the function $f(\cdot)$ that we wish to optimize. For example, consider inserting an element $e$ to a local solution $S_i$ which has high marginal contribution, meaning that $e$ increases the value $f(S_i)$ significantly. Then even if the hamming distance between $S_i$ and $S_i'$ is only one, we might want to send this updated solution to the central machine since this one element has a large impact.
