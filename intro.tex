\section{Introduction}

In many large datasets, it is useful to have a summary of its content. One way to build this summary is to select a set of representative samples. For example, given a large collection of images, it might be useful to have a small collection of images that represent the entire dataset. This representative set can be used to communicate an at-a-glance version of the dataset. In many settings, sequential greedy algorithms can find a near-optimal representative subset of input data. However, the volume of data often exceeds what could practically be stored on a single server. In this case, Mirzasoleiman, Baharan, Sarkar, and Krause show that when the objective function is submodular, near-optimal representatives of datasets can be found efficiently with a distributed greedy algorithm. In their 2013 NIPS paper, "Distributed submodular maximization: Identifying representative elements in massive data", they describe an approach to find these solutions, along with error bounds for several settings and results from experiments on several datasets.

	The main goal of this project is to extend their setting to account for insertions and deletions of elements to datasets. Another goal, if time permits, is to implement a reliable system where failure might occur. Insertions and deletions are relevant because few large datasets are completely static -- many applications with large, distributed data change over time as users or other processes generate new inputs. In addition, we believe that insertions and deletions cause new interesting challenges, both in terms of implementation and in terms of theoretical guarantees. We start by reviewing submodularity and the algorithm from Mirzasoleiman et al., and then move on to discussing the challenges of insertions, deletions, and failure.