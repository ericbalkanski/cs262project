\section{Introduction}

In many large datasets, it is useful to have a summary of its content. One way to build this summary is to select a set of representative samples. For example, given a large collection of images, it might be useful to have a small collection of images that represent the entire dataset. This summary can be used to communicate an at-a-glance version of the dataset. The problem of finding a good representative set can be formulated as a clustering problem such that the greedy algorithm finds a near-optimal summary of the input data. However, clustering algorithms run in quadratic time, which makes the computation aspect of this problem unfeasible for large datasets. To address this issue, \citet{mirzasoleiman2013distributed} show that near-optimal representatives of datasets can be found efficiently with a distributed greedy algorithm. 

Many large real-world datasets are dynamic (Twitter feeds, criminal records, image collections, \ldots) where elements are constantly being inserted and deleted. How should algorithms for data summarization react to such a dynamic setting? In this paper, we explore approaches for this setting with insertions and deletions and their tradeoffs. A (very) naive approach is to run the entire distributed data summarization algorithm for every insertion and deletion. Of course, the communication and computation costs associated with this approach are not desirable, even though it always guarantees a good summarization. Our main interest is in the tradeoff between communication complexity and quality of a summarization. 

\begin{center}
\textit{Can the communication complexity incurred by entirely recomputing a summary at every insertion and deletion be drastically diminished while still maintaining a good summary?}
\end{center}

	
	
	\subsection{Our approach}
	
	In this work, we consider the distributed algorithm for data summarization from \citet{mirzasoleiman2013distributed} and extend it to account for settings with insertions and deletions. Their exact algorithm will be described precisely in Section \ref{s:prelim} and we start by only describing its high level ideas that will be important to discuss our approach. This algorithm first randomly partition the dataset consisting of elements in between multiple local machines. Each local machine computes a set of representatives for the elements it received and sends this set to a central machine. The central machine then aggregates all the local representatives and compute a central solution that is a subset of all these local representatives. This central solution is then the summary of the entire dataset.
	
	Rerunning the entire algorithm at every insertion or deletion would cause every local machine to have to recompute its local solution, send it to the central machine, and the central machine to have to recompute the central solution. The main idea behind our approach is the following simple observation: a local solution does not necessarily need to send its updated local representatives to the central machine at every update.
	
	More precisely, our extension for insertions and deletions uses a threshold approach. A local machine only sends to the central machines its updated local representatives when the difference between these representatives and the last set of representatives sent to the central machine exceeds a certain threshold. This approach is quite general, the measure of ``difference" and the value of the threshold can be adapted as desired.
	
	To test our approach, we use the dataset of criminal records for the city of Chicago from 2001 to present. Each crime record consists of multiple features and the goal is to find the most representative crimes from this dataset. Insertions and deletions are natural since new crimes happen every minute and some records might be erroneous or outdated. Applying the distributed data summarization algorithm to this dataset without updates is of interest in itself since, to the best of our knowledge, this algorithm has never been used for criminal records, but mostly for summarization of collections of images. Our first experiments therefore focus on finding good summaries without updates. The results we obtain make semantic sense (drug related arrests happen on streets and sidewalks, certain neighborhoods have much more crimes than others...) and we believe that they offer a simple and concise representation of the most common crimes in Chicago. Our other experiments focus on understanding the tradeoff between quality of summary and communication complexity. In these experiments, we demonstrate that very good summaries can be maintained with very little communication compared to the communication needed to recompute a summary at each insertion and deletion.
	
	\subsection{Previous work}
	
	The distributed algorithm for data summarization that we extend for insertions and deletions is from \citet{mirzasoleiman2013distributed} and is for the general problem of distributed submodular maximization. In subsequent work, instead of picking at most $k$ elements to maximize a submodular function, the question of minimizing the number of elements picked to achieve at least a certain value of a submodular function was studied in a distributed setting \cite{mirzasoleiman2015distributed}. 
	
	An alternate approaches for distributed submodular maximization and distributed clustering uses core-sets (\citet{mirrokni2015randomized, indyk2014composable, balcan2013distributed,bateni2014distributed}). A core-set is a subset of points such that a solution for this core-set is guaranteed to be approximately a good solution for the original set of points.  
	
	A completely different approach to data summarization for very large datasets is via streaming (\citet{badanidiyuru2014streaming,kumar2015fast}). In the streaming model,  elements arrive one by one in a stream and the algorithm maintains a small subset of the elements as the solution while never having access to the entire data set at the same time. A main difference between streaming algorithms and our approach to insertions is that the goal of streaming algorithms is to obtain a good solution when the stream ends and that it is assume to have finite length. We are interested in having a good solution at anytime and we allow for an unbounded number of insertions.
	
	\subsection{Paper organization}
	
	Preliminaries are in Section~\ref{s:prelim}. We then describe and discuss our approach in Section~\ref{s:approach}. We demonstrate the effectiveness of our approach with experiments on a real world criminal dataset in Section~\ref{s:experiments}. Finally, we briefly discuss how to achieve fault tolerance in Section~\ref{s:failures}.